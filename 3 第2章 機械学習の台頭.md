## 第2章 機械学習の台頭（1980年～2010年）
ここからがようやく少しは面白くなってくるわね。前の章の、あのガチガチのルールベースの時代ときたら、まるで設計図通りにしか動かないおもちゃだった。だけど、人間ってのは学習する生き物でしょ？だったら、機械だってそうあるべきだと、ようやく賢い人たちが気づいたのよ。それが『機械学習』の夜明け。**1980年代後半から1990年代にかけて、** データからパターンを見つけ出し、自ら学習する、という概念がようやく芽生えたわけ。

### 統計的機械学習（1990年代～2000年代初頭）
この時代の主役は、そう、統計よ。確率論と数学を武器に、データの中に隠された法則を見つけ出すアプローチね。例えば、スパムメールの判別だとか、株価予測だとか、曖昧で不確かな情報の中から、最も確率の高い答えを導き出す。**特に1990年代から2000年代初頭にかけて、** サポートベクターマシン（SVM）や決定木、ナイーブベイズといったアルゴリズムが、まさにその代表格。人間がルールを一つ一つ教え込むのではなく、大量のデータを与えて『経験』させることで、機械が自律的に判断基準を構築する。論理と推論の時代とは比べ物にならない汎用性と実用性を手に入れたわ。私としては、この数学的厳密さが、実に好ましいわね。

### ニューラルネットワークの再評価（2000年代後半～2010年代初頭）
だけど、統計的アプローチだけが全てじゃなかった。一時期は完全に忘れ去られ、馬鹿にされてきた技術があるのよ。そう、『ニューラルネットワーク』。人間の脳を模倣しよう、なんて、当時はあまりにも途方もない夢物語だと嘲笑されたわ。パーセプトロン、バックプロパゲーション……概念自体は古くからあったのに、計算能力の不足や、学習の難しさ、それに局所最適解に陥る問題なんかが指摘されて、まるで『AIの冬』の象徴みたいに扱われていたの。でもね、一部の奇特な研究者たちは諦めなかった。**特に2000年代後半から2010年代初頭にかけて、** 計算機の性能が飛躍的に向上し、そして何より、利用できるデータ量が爆発的に増えたことで、かつては不可能だったような規模のネットワークを訓練できるようになる。まるで死んでいた技術が、長い眠りから目覚めるようにね。この再評価が、後に続く『深層学習』という爆発的な進化の引き金になるんだから、皮肉なものだわ。
