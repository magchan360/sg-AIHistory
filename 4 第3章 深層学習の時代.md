## 第3章 深層学習の時代（2010年代～現在）
やっとよ。機械学習の時代に片鱗を見せたニューラルネットワークが、まさに『覚醒』する時が来たわ。計算能力とデータ量の爆発的な増加、そして巧妙なアルゴリズムの進化が組み合わさって、AIはかつてない飛躍を遂げた。そう、これが『深層学習（ディープラーニング）』の時代よ。

### 画像認識とCNN（2012年～）
人間の視覚を模倣するなんて、かつては夢物語だった。だけど、この深層学習はそれを現実のものにしたわ。特に重要な役割を果たしたのが、『畳み込みニューラルネットワーク（CNN）』よ。画像から特徴を自動的に学習する多層構造を持つことで、手作業で特徴量を設計する必要がなくなったの。**2012年のImageNetコンペティションで、** このCNNが圧倒的な性能を示して以来、画像認識の分野は劇的に進歩した。物体検出、顔認識、自動運転…数年前まではSFの世界だったことが、まるで当然のように実現されていったわ。目に見える世界を機械が『理解』し始めた、記念すべき瞬間だったのよ。
※ CNN：Convolutional Neural Network


### 自然言語処理とRNN/Transformer（2010年代中盤～）
画像に続いて、機械が人間の言葉を理解し、生成する能力も飛躍的に向上したわ。この分野で初期に活躍したのが、『リカレントニューラルネットワーク（RNN）』、特に『LSTM』や『GRU』といった改良型よ。これらは文章のような「時系列データ」の扱いに長けていて、翻訳や音声認識、テキスト生成といった分野で目覚ましい成果を上げた。だけど、長文になると情報の保持が難しくなる、という根本的な課題があった。
※ RNN：Recurrent Neural Network

そして、その問題を打破し、現在のAIエージェントの礎を築いたのが、**2017年に登場した『Transformer』モデル** よ。これは『Attention（注意機構）』という革新的なメカニズムを使って、文中の単語間の関係性を効率的に捉えることを可能にした。さらに、並列処理に適していたため、それまでのモデルでは不可能だったような大規模なネットワークの訓練が可能になったの。BERTやGPTといった大規模言語モデル（LLM）の登場は、まさにこのTransformerの恩恵よ。これにより、機械は単語の羅列を処理するだけでなく、言葉のニュアンスや文脈、さらには常識的な知識までをも学習し、人間のような自然な対話や文章生成を可能にした。ようやく、言葉の壁が少しずつ崩れ始めた、ということね。